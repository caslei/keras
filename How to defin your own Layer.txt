实现自己的Keras层
2017年10月21日 23:05:06 DawnRanger 阅读数：1959
个人分类： deep-learning Python

一. 所有keras层的基类:Layer

keras的所有层的基类定义在keras/engine/topology.py文件中的Layer类中。
python语言基础

用到的装饰器：
    @property 让类函数能像类变量一样操作
    @interfaces.legacy_xxx_support 让函数支持keras 1.x的 API
    @classmothod 类函数，属于整个类，类似于C++/JAVA中的静态函数。类方法有类变量cls传入，从而可以用cls做一些相关的处理。子类继承时，调用该类方法时，传入的类变量cls是子类，而非父类。既可以在类内部使用self访问，也可以通过实例、类名访问。
    @staticmethod 将外部函数集成到类体中,既可以在类内部使用self访问，也可以通过实例、类名访问。基本上等同于一个全局函数。

magic函数:
    __call__ 让类的实例可以像函数一样调用，正是python的这种特性让我们可以像这样进行层之间的连接：

inputs = Input(shape=(784,))
x = Dense(64, activation='relu')(inputs) 
# 前面的Dense(64, activation='relu')生成了类Dense的一个实例
# 后面的(input)将调用类Dense的__call__函数

InputSpec: 确定层的ndim,dtype,shape，每一层都应有一个input_spec属性，保存InputSpec实例的list(每一个输入tensor都对应一个)

重点关注以下函数
1. add_weight
每层的参数通过这个函数来设定。可以看到它最终调用的是 K.variable 来生成变量，打开 keras/backend/tensorflow_backend.py 可以看到它生成变量的方式： v = tf.Variable(value, dtype=tf.as_dtype(dtype), name=name)
让人惊讶的是，keras从居然不是使用tf.get_variable的方式生成变量，可见keras在设计时就根本没有考虑到变量共享，从之前的经验来看，要用keras设计多GPU程序是非常棘手的。(要想让Keras支持多GPU并行，必须从这一步开始修改代码，而这里已经是keras非常底层的代码了。)

2. call / __call__
call是最重要的函数，它用于实现层的功能，子类必须实现。
魔法函数 __call__ 会将收到的输入传递给 call 函数，然后调用 call 函数实现具体的功能。

3. comput_output_shape
根据input_shape 计算输出的shape，子类必须实现。用于自动推断下一层的输入尺寸。

4. build
用来创建当前层的weights，子类必须实现。

5. get_config / from_config
get_config 返回一个字典，获取当前层的参数信息。
from_config 使用根据参数生成一个新的层。代码只有一行：

@classmethod
def from_config(cls, config):
    return cls(**config)

可见from_config是一个classmethod，根据传入的参数，使用当前类的构造函数来生成一个实例。通过子类调用时，cls是子类而不是基类Layer。


二、实现自己的keras层

官方文档的说明：
对于简单的定制操作，我们或许可以通过使用layers.core.Lambda层来完成。但对于任何具有可训练权重的定制层，你应该自己来实现。

    build(input_shape)：这是定义权重的方法，可训练的权应该在这里被加入列表self.trainable_weights中。其他的属性还包括self.non_trainabe_weights（列表）和self.updates（需要更新的形如（tensor, new_tensor）的tuple的列表）。你可以参考BatchNormalization层的实现来学习如何使用上面两个属性。这个方法必须设置self.built = True，可通过调用super([layer],self).build()实现
    call(x)：这是定义层功能的方法，除非你希望你写的层支持masking，否则你只需要关心call的第一个参数：输入张量
    compute_output_shape(input_shape)：如果你的层修改了输入数据的shape，你应该在这里指定shape变化的方法，这个函数使得Keras可以做自动shape推断

代码示例(来自keras代码库)
最简单的层Activation层(没有参数)：

class Activation(Layer):
    def __init__(self, activation, **kwargs):
        super(Activation, self).__init__(**kwargs)
        self.supports_masking = True
        self.activation = activations.get(activation)

    def call(self, inputs):
        return self.activation(inputs)

    def get_config(self):
        config = {'activation': activations.serialize(self.activation)}
        base_config = super(Activation, self).get_config()
        return dict(list(base_config.items()) + list(config.items()))

Dropout层：

class Dropout(Layer):
    @interfaces.legacy_dropout_support
    def __init__(self, rate, noise_shape=None, seed=None, **kwargs):
        super(Dropout, self).__init__(**kwargs)
        self.rate = min(1., max(0., rate))
        self.noise_shape = noise_shape
        self.seed = seed
        self.supports_masking = True

    def _get_noise_shape(self, inputs):
        if self.noise_shape is None:
            return self.noise_shape

        symbolic_shape = K.shape(inputs)
        noise_shape = [symbolic_shape[axis] if shape is None else shape
                       for axis, shape in enumerate(self.noise_shape)]
        return tuple(noise_shape)

    def call(self, inputs, training=None):
        if 0. < self.rate < 1.:
            noise_shape = self._get_noise_shape(inputs)

            def dropped_inputs():
                return K.dropout(inputs, self.rate, noise_shape,
                                 seed=self.seed)
            return K.in_train_phase(dropped_inputs, inputs,
                                    training=training)
        return inputs

    def get_config(self):
        config = {'rate': self.rate,
                  'noise_shape': self.noise_shape,
                  'seed': self.seed}
        base_config = super(Dropout, self).get_config()
        return dict(list(base_config.items()) + list(config.items()))

Dense 层：

class Dense(Layer):
    @interfaces.legacy_dense_support
    def __init__(self, units,
                 activation=None,
                 use_bias=True,
                 kernel_initializer='glorot_uniform',
                 bias_initializer='zeros',
                 kernel_regularizer=None,
                 bias_regularizer=None,
                 activity_regularizer=None,
                 kernel_constraint=None,
                 bias_constraint=None,
                 **kwargs):
        if 'input_shape' not in kwargs and 'input_dim' in kwargs:
            kwargs['input_shape'] = (kwargs.pop('input_dim'),)
        super(Dense, self).__init__(**kwargs)
        self.units = units
        self.activation = activations.get(activation)
        self.use_bias = use_bias
        self.kernel_initializer = initializers.get(kernel_initializer)
        self.bias_initializer = initializers.get(bias_initializer)
        self.kernel_regularizer = regularizers.get(kernel_regularizer)
        self.bias_regularizer = regularizers.get(bias_regularizer)
        self.activity_regularizer = regularizers.get(activity_regularizer)
        self.kernel_constraint = constraints.get(kernel_constraint)
        self.bias_constraint = constraints.get(bias_constraint)
        self.input_spec = InputSpec(min_ndim=2)
        self.supports_masking = True

    def build(self, input_shape):
        assert len(input_shape) >= 2
        input_dim = input_shape[-1]

        self.kernel = self.add_weight(shape=(input_dim, self.units),
                                      initializer=self.kernel_initializer,
                                      name='kernel',
                                      regularizer=self.kernel_regularizer,
                                      constraint=self.kernel_constraint)
        if self.use_bias:
            self.bias = self.add_weight(shape=(self.units,),
                                        initializer=self.bias_initializer,
                                        name='bias',
                                        regularizer=self.bias_regularizer,
                                        constraint=self.bias_constraint)
        else:
            self.bias = None
        self.input_spec = InputSpec(min_ndim=2, axes={-1: input_dim})
        self.built = True

    def call(self, inputs):
        output = K.dot(inputs, self.kernel)
        if self.use_bias:
            output = K.bias_add(output, self.bias)
        if self.activation is not None:
            output = self.activation(output)
        return output

    def compute_output_shape(self, input_shape):
        assert input_shape and len(input_shape) >= 2
        assert input_shape[-1]
        output_shape = list(input_shape)
        output_shape[-1] = self.units
        return tuple(output_shape)

    def get_config(self):
        config = {
            'units': self.units,
            'activation': activations.serialize(self.activation),
            'use_bias': self.use_bias,
            'kernel_initializer': initializers.serialize(self.kernel_initializer),
            'bias_initializer': initializers.serialize(self.bias_initializer),
            'kernel_regularizer': regularizers.serialize(self.kernel_regularizer),
            'bias_regularizer': regularizers.serialize(self.bias_regularizer),
            'activity_regularizer': regularizers.serialize(self.activity_regularizer),
            'kernel_constraint': constraints.serialize(self.kernel_constraint),
            'bias_constraint': constraints.serialize(self.bias_constraint)
        }
        base_config = super(Dense, self).get_config()
        return dict(list(base_config.items()) + list(config.items()))


#=================================================================================
#=================================================================================


[Keras] 使用Keras编写自定义网络层（layer）
2018年02月09日 10:27:34 LandH的Blog 阅读数：5178 标签： DeepLearning Keras Custom
个人分类： 神经网络&Deep Learning

Keras提供众多常见的已编写好的层对象，例如常见的卷积层、池化层等，我们可以直接通过以下代码调用：

# 调用一个Conv2D层
from keras import layers
conv2D = keras.layers.convolutional.Conv2D(filters,\
					kernel_size, \
					strides=(1, 1), \
					padding='valid', \
					...)

但是在实际应用中，我们经常需要自己构建一些层对象，满足某些自定义网络的特殊需求。
幸运的是，Keras对自定义层提供了良好的支持。

下面对常用方法进行总结。

方法1：keras.core.lambda()
如果我们的自定义层中不包含可训练的权重，而只是对上一层输出做一些函数变换，那么我们可以直接使用keras.core模块（该模块包含常见的基础层，如Dense、Activation等）下的lambda函数：

keras.layers.core.Lambda(function, output_shape=None, mask=None, arguments=None)

参数说明：
function：要实现的函数，该函数仅接受一个变量，即上一层的输出
output_shape：函数应该返回的值的shape，可以是一个tuple，也可以是一个根据输入shape计算输出shape的函数
mask: 掩膜
arguments：可选，字典，用来记录向函数中传递的其他关键字参数

但是多数情况下，我们需要定义的是一个全新的、拥有可训练权重的层，这个时候我们就需要使用下面的方法。


方法2: 编写Layer继承类

keras.engine.topology中包含了Layer的父类，我们可以通过继承来实现自己的层。 要定制自己的层，需要实现下面三个方法

build(input_shape)：这是定义权重的方法，可训练的权应该在这里被加入列表self.trainable_weights中。其他的属性还包括self.non_trainabe_weights（列表）和self.updates（需要更新的形如（tensor,new_tensor）的tuple的列表）。这个方法必须设置self.built = True，可通过调用super([layer],self).build()实现。

call(x)：这是定义层功能的方法，除非你希望你写的层支持masking，否则你只需要关心call的第一个参数：输入张量。

compute_output_shape(input_shape)：如果你的层修改了输入数据的shape，你应该在这里指定shape变化的方法，这个函数使得Keras可以做自动shape推断。

一个比较好的学习方法是阅读Keras已编写好的类的源代码，尝试理解其中的逻辑。

下面，我们将通过一个实际的例子，编写一个自定义层。
出于学习的目的，在该例子中，会添加一些的注释文字，用以解释一些函数功能。

该层结构来源自DenseNet，代码参考Github。

from keras.layers.core import Layer
from keras.engine import InputSpec
from keras import backend as K
try:
    from keras import initializations
except ImportError:
    from keras import initializers as initializations
# 继承父类Layer
class Scale(Layer):
    '''
    该层功能：
        通过向量元素依次相乘（Element wise multiplication）调整上层输出的形状。
        out = in * gamma + beta,
        gamma代表权重weights，beta代表偏置bias
    参数列表：
        axis: int型，代表需要做scale的轴方向，axis=-1 代表选取默认方向（横行）。
        momentum: 对数据方差和标准差做指数平均时的动量.
        weights: 初始权重，是一个包含两个numpy array的list, shapes:[(input_shape,), (input_shape,)]
        beta_init: 偏置量的初始化方法名。(参考Keras.initializers.只有weights未传参时才会使用.
        gamma_init: 权重量的初始化方法名。(参考Keras.initializers.只有weights未传参时才会使用.
    '''
    def __init__(self, weights=None, axis=-1, beta_init = 'zero', gamma_init = 'one', momentum = 0.9, **kwargs):
        # 参数**kwargs代表按字典方式继承父类
        self.momentum = momentum
        self.axis = axis
        self.beta_init = initializers.Zeros()
        self.gamma_init = initializers.Ones()
        self.initial_weights = weights
        super(Scale, self).__init__(**kwargs)

    def build(self, input_shape):
        self.input_spec = [InputSpec(shape=input_shape)]
        # 1：InputSpec(dtype=None, shape=None, ndim=None, max_ndim=None, min_ndim=None, axes=None)
        #Docstring:     
        #Specifies the ndim, dtype and shape of every input to a layer.
        #Every layer should expose (if appropriate) an `input_spec` attribute:a list of instances of InputSpec (one per input tensor).
        #A None entry in a shape is compatible with any dimension
        #A None shape is compatible with any shape.

        # 2:self.input_spec: List of InputSpec class instances
        # each entry describes one required input:
        #     - ndim
        #     - dtype
        # A layer with `n` input tensors must have
        # an `input_spec` of length `n`.

        shape = (int(input_shape[self.axis]),)

        # Compatibility with TensorFlow >= 1.0.0
        self.gamma = K.variable(self.gamma_init(shape), name='{}_gamma'.format(self.name))
        self.beta = K.variable(self.beta_init(shape), name='{}_beta'.format(self.name))

        self.trainable_weights = [self.gamma, self.beta]

        if self.initial_weights is not None:
            self.set_weights(self.initial_weights)
            del self.initial_weights

    def call(self, x, mask=None):
        input_shape = self.input_spec[0].shape
        broadcast_shape = [1] * len(input_shape)
        broadcast_shape[self.axis] = input_shape[self.axis]

        out = K.reshape(self.gamma, broadcast_shape) * x + K.reshape(self.beta, broadcast_shape)
        return out

    def get_config(self):
        config = {"momentum": self.momentum, "axis": self.axis}
        base_config = super(Scale, self).get_config()
        return dict(list(base_config.items()) + list(config.items()))

以上就是编写自定义层的实例，可以直接添加到自己的model中。
编写好的layer自动存放在custom_layers中，通过import调用。

from custom_layers import Scale

def myNet(growth_rate=32, \
nb_filter=64, \
reduction=0.0, \
dropout_rate=0.0, weight_decay=1e-4,...)

...
x = "last_layer_name"
x = Scale(axis=concat_axis, name='scale')(x)
...

model = Model(input, x, name='myNet')
return model




